{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG-U9aBN2kYO"
   },
   "source": [
    "# --> Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9A-__8Hy2kYR",
    "outputId": "4f46a769-6332-4c53-c5fc-43b438d4dc84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\karna\\anaconda3\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBOqjMFb8GKL"
   },
   "source": [
    "# --> Configuration des TPU Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_W1HXWYv8JfW",
    "outputId": "64cf0baa-4553-4bf0-d1af-3e82ab2f0e52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "  print(tpu_strategy)\n",
    "except ValueError:\n",
    "  tpu_strategy = None\n",
    "  #raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCBU2iny2kYT"
   },
   "source": [
    "# --> Importation dataset poeme de Victor Hugo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nubay9lq2kYT",
    "outputId": "60f7a3e2-083e-4ce5-edf5-bfad79a6a0e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du text :  127286\n",
      "Texte avant preprocessing :\n",
      " Parce que, jargonnant vêpres, jeûne et vigile,\n",
      "Exploitant Dieu qui rêve au fond du firmament,\n",
      "Vous a\n"
     ]
    }
   ],
   "source": [
    "if tpu_strategy is not None:\n",
    "    with open(\"victorhugo.txt\", \"r\", encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "    with open(\"../../Datasets/VictorHugoPoems/victorhugo.txt\", \"r\", encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "print(\"Taille du text : \", len(text))\n",
    "print(\"Texte avant preprocessing :\\n\", text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cuux3rec3VOg",
    "outputId": "82867566-4873-4fd8-dd6c-47388a26ab0b"
   },
   "outputs": [],
   "source": [
    "if tpu_strategy is not None:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONk_ljCO2kYU"
   },
   "source": [
    "# --> Preprocessing du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cGyc15ZJ2kYU",
    "outputId": "77ecc599-fa92-4a99-efe1-ebfa5853ec0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire :  57\n",
      "Vocabulaire :\n",
      " {\"'\", 'V', 'T', 'x', 'j', 'f', 'Q', 'Y', 'k', 'S', 'E', 'a', ':', 'u', '.', 'J', 's', 'i', 'F', 'A', '\\n', 'c', 'p', 'X', 'P', 'I', 'z', 'e', 'B', 'b', 'r', 'H', 'K', 'g', 'd', 'h', 'l', 'L', 'R', 'U', 'D', 'C', 'w', 'y', 'm', 'M', ' ', 'v', ',', 'n', '\"', 'N', 't', 'o', 'G', 'q', 'O'}\n",
      "Texte formate :\n",
      " Parce que, jargonnant vepres, jeune et vigile,\n",
      "Exploitant Dieu qui reve au fond du firmament,\n",
      "Vous a\n"
     ]
    }
   ],
   "source": [
    "#Supprime les caracteres inutiles, les majuscules...\n",
    "import unidecode\n",
    "text = unidecode.unidecode(text)\n",
    "text.lower()\n",
    "text = text.replace(\"2\", \"\")\n",
    "text = text.replace(\"1\", \"\")\n",
    "text = text.replace(\"8\", \"\")\n",
    "text = text.replace(\"5\", \"\")\n",
    "text = text.replace(\">\", \"\")\n",
    "text = text.replace(\"<\", \"\")\n",
    "text = text.replace(\"!\", \"\")\n",
    "text = text.replace(\"?\", \"\")\n",
    "text = text.replace(\"-\", \"\")\n",
    "text = text.replace(\"$\", \"\")\n",
    "text = text.replace(\";\", \"\")\n",
    "text = text.strip()\n",
    "\n",
    "#Supprime tous les doublons\n",
    "vocab = set(text) \n",
    "\n",
    "#Affichage resultat\n",
    "print(\"Taille du vocabulaire : \", len(vocab))\n",
    "print(\"Vocabulaire :\\n\", vocab)\n",
    "print(\"Texte formate :\\n\", text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtCsfNe22kYV",
    "outputId": "48d35b7d-e233-4223-826f-16f60524ad3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab to int :\n",
      " {\"'\": 0, 'V': 1, 'T': 2, 'x': 3, 'j': 4, 'f': 5, 'Q': 6, 'Y': 7, 'k': 8, 'S': 9, 'E': 10, 'a': 11, ':': 12, 'u': 13, '.': 14, 'J': 15, 's': 16, 'i': 17, 'F': 18, 'A': 19, '\\n': 20, 'c': 21, 'p': 22, 'X': 23, 'P': 24, 'I': 25, 'z': 26, 'e': 27, 'B': 28, 'b': 29, 'r': 30, 'H': 31, 'K': 32, 'g': 33, 'd': 34, 'h': 35, 'l': 36, 'L': 37, 'R': 38, 'U': 39, 'D': 40, 'C': 41, 'w': 42, 'y': 43, 'm': 44, 'M': 45, ' ': 46, 'v': 47, ',': 48, 'n': 49, '\"': 50, 'N': 51, 't': 52, 'o': 53, 'G': 54, 'q': 55, 'O': 56}\n",
      "Int to vocab :\n",
      " {0: \"'\", 1: 'V', 2: 'T', 3: 'x', 4: 'j', 5: 'f', 6: 'Q', 7: 'Y', 8: 'k', 9: 'S', 10: 'E', 11: 'a', 12: ':', 13: 'u', 14: '.', 15: 'J', 16: 's', 17: 'i', 18: 'F', 19: 'A', 20: '\\n', 21: 'c', 22: 'p', 23: 'X', 24: 'P', 25: 'I', 26: 'z', 27: 'e', 28: 'B', 29: 'b', 30: 'r', 31: 'H', 32: 'K', 33: 'g', 34: 'd', 35: 'h', 36: 'l', 37: 'L', 38: 'R', 39: 'U', 40: 'D', 41: 'C', 42: 'w', 43: 'y', 44: 'm', 45: 'M', 46: ' ', 47: 'v', 48: ',', 49: 'n', 50: '\"', 51: 'N', 52: 't', 53: 'o', 54: 'G', 55: 'q', 56: 'O'}\n"
     ]
    }
   ],
   "source": [
    "#On traduit maintenant tout le vocabulaire en nombre\n",
    "vocab_size = len(vocab)\n",
    "#Dictionnaire traduction\n",
    "vocab_to_int = {l:i for i,l in enumerate(vocab)} \n",
    "int_to_vocab = {i:l for i,l in enumerate(vocab)}\n",
    "#Affichage\n",
    "print(\"Vocab to int :\\n\", vocab_to_int)\n",
    "print(\"Int to vocab :\\n\", int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QadBhdDq2kYV",
    "outputId": "bfb512f6-80c1-4525-f754-5024aea41671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 11, 30, 21, 27, 46, 55, 13, 27, 48, 46, 4, 11, 30, 33, 53, 49, 49, 11, 49, 52, 46, 47, 27, 22, 30, 27, 16, 48, 46, 4, 27, 13, 49, 27, 46, 27, 52, 46, 47, 17, 33, 17, 36, 27, 48, 20, 10, 3, 22, 36, 53, 17, 52, 11, 49, 52, 46, 40, 17, 27, 13, 46, 55, 13, 17, 46, 30, 27, 47, 27, 46, 11, 13, 46, 5, 53, 49, 34, 46, 34, 13, 46, 5, 17, 30, 44, 11, 44, 27, 49, 52, 48, 20, 1, 53, 13, 16, 46, 11]\n"
     ]
    }
   ],
   "source": [
    "#Le dictionnaire nous permet de traduire notre text en nombre\n",
    "encoded = [vocab_to_int[l] for l in text]\n",
    "encoded_sentence = encoded[:100]\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isAiYALQ2kYW",
    "outputId": "36af362f-dfa2-4242-e541-bcf1597c681d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'a', 'r', 'c', 'e', ' ', 'q', 'u', 'e', ',', ' ', 'j', 'a', 'r', 'g', 'o', 'n', 'n', 'a', 'n', 't', ' ', 'v', 'e', 'p', 'r', 'e', 's', ',', ' ', 'j', 'e', 'u', 'n', 'e', ' ', 'e', 't', ' ', 'v', 'i', 'g', 'i', 'l', 'e', ',', '\\n', 'E', 'x', 'p', 'l', 'o', 'i', 't', 'a', 'n', 't', ' ', 'D', 'i', 'e', 'u', ' ', 'q', 'u', 'i', ' ', 'r', 'e', 'v', 'e', ' ', 'a', 'u', ' ', 'f', 'o', 'n', 'd', ' ', 'd', 'u', ' ', 'f', 'i', 'r', 'm', 'a', 'm', 'e', 'n', 't', ',', '\\n', 'V', 'o', 'u', 's', ' ', 'a']\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = [int_to_vocab[i] for i in encoded_sentence]\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYryst7-2kYW",
    "outputId": "0b0fd324-720a-4630-bdc5-5ae0ea7da250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parce que, jargonnant vepres, jeune et vigile,\n",
      "Exploitant Dieu qui reve au fond du firmament,\n",
      "Vous a\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = \"\".join(decoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq5tJNSa2kYX"
   },
   "source": [
    "# --> Creation des batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6OWvihXK2kYX",
    "outputId": "80d433df-129c-45cf-d265-18432fc89662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First inputs :  [24, 11, 30, 21, 27, 46, 55, 13, 27, 48]\n",
      "First targets :  [11, 30, 21, 27, 46, 55, 13, 27, 48, 46]\n"
     ]
    }
   ],
   "source": [
    "#Un batch = plusieurs sequences de mots\n",
    "#Ce qu'on peut faire lorsqu'on a un dataset comme cela, on peut prendre une sequence de quelques mots\n",
    "#Chaque lettre est une entree dont le target est la lettre suivante. \n",
    "#Une incoherence peut arriver lors de l'analyse de la premiere lettre d'une sequence\n",
    "#Car dans notre cellule RNN il n'a pas d'informations sur la lettre precedente car la memoire est nulle.\n",
    "#Au lieu de lui mettre un etat nulle on lui mets l'etat retenu du batch precendent.\n",
    "#On ne peut donc pas se permettre de selectionner des sequences aleatoires dans notre texte.\n",
    "#On va donc seprarer notre texte en chunks\n",
    "#Une epoch : un ensemble de batch\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def gen_batch(inputs, targets, seq_len, batch_size, noise=0):\n",
    "    \n",
    "    chunk_size = (len(inputs) -1) // batch_size\n",
    "    sequences_per_chunk = chunk_size // seq_len\n",
    "    \n",
    "    for seq in range(0, sequences_per_chunk):\n",
    "        batch_inputs = np.zeros((batch_size, seq_len))\n",
    "        batch_targets = np.zeros((batch_size, seq_len))\n",
    "        for b in range(0, batch_size):\n",
    "            fr = (b*chunk_size) + (seq*seq_len)\n",
    "            to = fr + seq_len\n",
    "            batch_inputs[b] = inputs[fr:to]\n",
    "            batch_targets[b] = inputs[fr+1:to+1]\n",
    "\n",
    "            if noise > 0: #\"noise\" aide le model a generaliser, evite l'overfitting\n",
    "                noise_indices = np.random.choice(seq_len, noise)\n",
    "                batch_inputs[b][noise_indices] = np.random.randint(0, vocab_size)\n",
    "\n",
    "        yield batch_inputs, batch_targets #Permet d'appeler la fonction dans la boucle\n",
    "            \n",
    "inputs, targets = encoded, encoded[1:]\n",
    "print(\"First inputs : \", inputs[:10])\n",
    "print(\"First targets : \", targets[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-PN-0nZ2kYY",
    "outputId": "208467eb-4b6d-4c8d-bc9c-a498db4465e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################### Sans noise #####################\n",
      "\n",
      "----------------------Step  1 ----------------------\n",
      "\n",
      "Batch input :\n",
      " (32, 5) \n",
      "Batch target shape :\n",
      " (32, 5)\n",
      "\n",
      "Batch input :\n",
      " [24. 11. 30. 21. 27.] \n",
      "Batch target :\n",
      " [11. 30. 21. 27. 46.]\n",
      "\n",
      "----------------------Step  2 ----------------------\n",
      "\n",
      "Batch input :\n",
      " (32, 5) \n",
      "Batch target shape :\n",
      " (32, 5)\n",
      "\n",
      "Batch input :\n",
      " [46. 55. 13. 27. 48.] \n",
      "Batch target :\n",
      " [55. 13. 27. 48. 46.]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n##################### Sans noise #####################\")\n",
    "i = 0\n",
    "for batch_inputs, batch_targets in gen_batch(inputs, targets, seq_len=5, batch_size=batch_size, noise=0): #Sequence de 5, batch de 64\n",
    "    i += 1\n",
    "    print(\"\\n----------------------Step \", i, \"----------------------\")\n",
    "    print(\"\\nBatch input :\\n\", batch_inputs.shape, \"\\nBatch target shape :\\n\", batch_targets.shape)\n",
    "    print(\"\\nBatch input :\\n\", batch_inputs[0], \"\\nBatch target :\\n\", batch_targets[0])\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmwXhMOV2kYY",
    "outputId": "4e5443a8-85b8-42e3-acec-7ae612dfd8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################### Avec noise = 3 #####################\n",
      "\n",
      "---------------------- Step  1  ----------------------\n",
      "\n",
      "Batch input :\n",
      " (32, 5) \n",
      "Batch target shape :\n",
      " (32, 5)\n",
      "\n",
      "Batch input :\n",
      " [24. 42. 30. 42. 27.] \n",
      "Batch target :\n",
      " [11. 30. 21. 27. 46.]\n",
      "\n",
      "---------------------- Step  2  ----------------------\n",
      "\n",
      "Batch input :\n",
      " (32, 5) \n",
      "Batch target shape :\n",
      " (32, 5)\n",
      "\n",
      "Batch input :\n",
      " [30. 55. 13. 30. 48.] \n",
      "Batch target :\n",
      " [55. 13. 27. 48. 46.]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n##################### Avec noise = 3 #####################\")\n",
    "i = 0\n",
    "for batch_inputs, batch_targets in gen_batch(inputs, targets, seq_len=5, batch_size=batch_size, noise=3): #Sequence de 5, batch de 64\n",
    "    i += 1\n",
    "    print(\"\\n---------------------- Step \", i, \" ----------------------\")\n",
    "    print(\"\\nBatch input :\\n\", batch_inputs.shape, \"\\nBatch target shape :\\n\", batch_targets.shape)\n",
    "    print(\"\\nBatch input :\\n\", batch_inputs[0], \"\\nBatch target :\\n\", batch_targets[0])\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhS45JOz2kYZ"
   },
   "source": [
    "# --> One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9oXL8vKZ2kYZ"
   },
   "outputs": [],
   "source": [
    "#Les valeurs au dessus ne nous convienne pas pour entrainer un model il y a mieux.\n",
    "#On va donc utiliser le one hot encoding pour simplifier la tache à notre model.\n",
    "#Exemple de one hot encoding : a => 2 => [0, 1, 0, 0]\n",
    "#Le one hot encoding est tres efficace lorsqu'on veut specifier des classes.\n",
    "#En effet, il n'y a aucune raison qu'un nombres que nous donnons a un caractere\n",
    "#ait un nombre plus eleve et donc avec plus de poids qu'un autre alors qu'il n'y\n",
    "#a aucune hierarchie entre les caracteres.\n",
    "class OneHot(tf.keras.layers.Layer): #On creer une custom layer OneHot\n",
    "    \n",
    "    def __init__(self, depth, **kwargs):\n",
    "        super(OneHot, self).__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "         #Transforme le x en int 32 et creer un vecteur one hot encoded\n",
    "        return tf.one_hot(tf.cast(x, tf.int32), self.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8qO9puz2kYZ",
    "outputId": "3f6d2205-c36e-4920-c88d-99cb7802dbd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input letter :\n",
      " 24.0\n",
      "Next letter prediction :\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "class RNNModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.one_hot = OneHot(len(vocab))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        output_layer = self.one_hot(inputs)\n",
    "        return output_layer\n",
    "    \n",
    "batch_inputs, batch_targets = next(gen_batch(inputs, targets, seq_len=50, batch_size=batch_size)) #64 sequences, 50 elements\n",
    "model = RNNModel(len(vocab))\n",
    "output = model.predict(batch_inputs)[0][0]\n",
    "\n",
    "print(\"Input letter :\\n\", batch_inputs[0][0])\n",
    "print(\"Next letter prediction :\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FegMimzq2kYa"
   },
   "source": [
    "# --> Creation du model RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3SdJOa_2kYb",
    "outputId": "1e65415c-8aa1-464b-db5b-652b83197b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(32, None, 57), dtype=tf.float32, name=None), name='one_hot_1/one_hot:0', description=\"created by layer 'one_hot_1'\")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "def create_model():\n",
    "  #Input layer\n",
    "  #On ne set pas le nombre d'element dans les sequences\n",
    "  tf_inputs = tf.keras.Input(shape=(None,), batch_size=batch_size)\n",
    "  #One hot layer\n",
    "  #En lui passant tf_inputs, on specifie la shape qu'on enverra dans la layer one_hot\n",
    "  one_hot = OneHot(vocab_size)(tf_inputs) \n",
    "  print(one_hot)\n",
    "  #LSTM layers\n",
    "  #\"return_sequences\" permet de specifier que l'on prend en compte plusieurs des anciennes\n",
    "  #cellules LSTM, si on met a False nous aurions que l'information de la derniere cellule LSTM\n",
    "  #\"stateful\" permet de specifier qu'a chaque appel on ne va pas reinitialiser les cellules.\n",
    "  #A chaque appel l'etat initial sera egal au dernier element de la sequence precedente\n",
    "  if tpu_strategy is None:\n",
    "    rnn_layer1 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(one_hot) \n",
    "    rnn_layer2 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(rnn_layer1)\n",
    "  else:\n",
    "    rnn_layer1 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=False)(one_hot)    #Google Colab TPU\n",
    "    rnn_layer2 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=False)(rnn_layer1) #Google Colab TPU\n",
    "  #Dense layer\n",
    "  hidden_layer = tf.keras.layers.Dense(128, activation=\"relu\")(rnn_layer2)\n",
    "  #Output layer\n",
    "  output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(hidden_layer)\n",
    "  model = tf.keras.Model(inputs=tf_inputs, outputs=output_layer)\n",
    "  return model\n",
    "\n",
    "if tpu_strategy is None: \n",
    "  model = create_model() #Local model\n",
    "else:\n",
    "  with tpu_strategy.scope(): \n",
    "    model = create_model() #TPU Google Colab Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "toDMEy1a2kYc",
    "outputId": "a2c41118-4644-45e5-a59a-6a265f5650c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input shape :  (32, 50)\n",
      "First prediction :\n",
      " [0.0175288  0.01752265 0.01757541 0.01756948 0.01753305 0.01763293\n",
      " 0.01752326 0.01750845 0.01749237 0.01763991 0.01756651 0.01748828\n",
      " 0.01750534 0.01744902 0.01753117 0.01748647 0.017477   0.01747479\n",
      " 0.01754663 0.01755825 0.01758766 0.01756385 0.01752383 0.01756142\n",
      " 0.01756185 0.01748588 0.0174942  0.01756116 0.01754531 0.01750287\n",
      " 0.01755021 0.01748655 0.01758426 0.01754696 0.01755339 0.01763955\n",
      " 0.01755495 0.01752679 0.01754701 0.01753615 0.01752554 0.01758143\n",
      " 0.01757491 0.01756363 0.01750428 0.01764829 0.017561   0.0175184\n",
      " 0.01757929 0.01761205 0.01753645 0.01752489 0.01753761 0.01758648\n",
      " 0.01750899 0.01756787 0.0175453 ]\n",
      "Second prediction :\n",
      " [0.0175288  0.01752265 0.01757541 0.01756948 0.01753305 0.01763293\n",
      " 0.01752326 0.01750845 0.01749237 0.01763991 0.01756651 0.01748828\n",
      " 0.01750534 0.01744902 0.01753117 0.01748647 0.017477   0.01747479\n",
      " 0.01754663 0.01755825 0.01758766 0.01756385 0.01752383 0.01756142\n",
      " 0.01756185 0.01748588 0.0174942  0.01756116 0.01754531 0.01750287\n",
      " 0.01755021 0.01748655 0.01758426 0.01754696 0.01755339 0.01763955\n",
      " 0.01755495 0.01752679 0.01754701 0.01753615 0.01752554 0.01758143\n",
      " 0.01757491 0.01756363 0.01750428 0.01764829 0.017561   0.0175184\n",
      " 0.01757929 0.01761205 0.01753645 0.01752489 0.01753761 0.01758648\n",
      " 0.01750899 0.01756787 0.0175453 ]\n"
     ]
    }
   ],
   "source": [
    "#Reset les cellules du RNN\n",
    "model.reset_states()\n",
    "\n",
    "#Creer un premier batch\n",
    "batch_inputs, target_inputs = next(gen_batch(inputs, targets, seq_len=50, batch_size=batch_size))\n",
    "\n",
    "#Prediction pour un premier batch\n",
    "print(\"Batch input shape : \", batch_inputs.shape)\n",
    "outputs = model.predict(batch_inputs)\n",
    "#Prediction de la premiere sortie\n",
    "first_prediction = outputs[0][0]\n",
    "print(\"First prediction :\\n\", first_prediction)\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "#Deuxieme prediction, c'est exactement la meme grace au stateful=True\n",
    "outputs = model.predict(batch_inputs)\n",
    "second_prediction = outputs[0][0]\n",
    "print(\"Second prediction :\\n\", second_prediction)\n",
    "\n",
    "#Check si les deux predictions sont egales avec un reset_state() entre les deux\n",
    "assert(set(first_prediction)==set(second_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "avTViUqe2kYc"
   },
   "outputs": [],
   "source": [
    "if tpu_strategy is None:\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "  optimizer = tf.keras.optimizers.Adam(lr=0.001) #lr : learning rate\n",
    "  train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "else:\n",
    "  with tpu_strategy.scope():\n",
    "    loss_object = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.001) #lr : learning rate\n",
    "    train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Egz8-3VB2kYd"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #Fait une prediction sur le batch\n",
    "        predictions = model(inputs)\n",
    "        #Recupere l'erreur par rapport aux predictions faites\n",
    "        if tpu_strategy is None:\n",
    "          loss = loss_object(targets, predictions)\n",
    "        else:\n",
    "          loss = tf.reduce_sum(loss_object(targets, predictions))\n",
    "    #Calcul du gradient\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    #Change les poids du model grace au gradient\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    #Garde l'information sur l'evolution de l'entrainement\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets, predictions)\n",
    "\n",
    "@tf.function\n",
    "def predict(inputs):\n",
    "    # Fait une prediction sur tous le batch\n",
    "    predictions = model(inputs)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yl3DLSbb2kYd"
   },
   "source": [
    "# --> Entrainement du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQ9iiUWM2kYd",
    "outputId": "035b0bf6-9b09-4672-c8ce-0944284c4d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(32, None)]              0         \n",
      "_________________________________________________________________\n",
      "one_hot_1 (OneHot)           (32, None, 57)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (32, None, 128)           95232     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (32, None, 128)           131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 128)           16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (32, None, 57)            7353      \n",
      "=================================================================\n",
      "Total params: 250,681\n",
      "Trainable params: 250,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Epoch 2, Train Loss: 3.122755765914917, Train Accuracy: 15.5216350555419929"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-05a293283257>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m       \u001b[1;31m#Pendant toute cette etape dans le for, on ne reinitialise pas les states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_targets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#Sequence de taille 100, batch de 64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m           \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m       \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\r Epoch {}, Train Loss: {}, Train Accuracy: {}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m       print(template.format(epoch, \n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.reset_states()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if tpu_strategy is None:\n",
    "  for epoch in range(10000):\n",
    "      #Pendant toute cette etape dans le for, on ne reinitialise pas les states\n",
    "      for batch_inputs, batch_targets in gen_batch(inputs, targets, seq_len=100, batch_size=batch_size, noise=0): #Sequence de taille 100, batch de 64\n",
    "          train_step(batch_inputs, batch_targets)\n",
    "      template = '\\r Epoch {}, Train Loss: {}, Train Accuracy: {}'\n",
    "      print(template.format(epoch, \n",
    "                            train_loss.result(), \n",
    "                            train_accuracy.result()*100), end=\"\")\n",
    "      model.reset_states() #On reinitialise le state pour la prochaine epoch\n",
    "else:\n",
    "  with tpu_strategy.scope():\n",
    "    for epoch in range(10000):\n",
    "        #Pendant toute cette etape dans le for, on ne reinitialise pas les states\n",
    "        for batch_inputs, batch_targets in gen_batch(inputs, targets, seq_len=100, batch_size=batch_size, noise=0): #Sequence de taille 100, batch de 64\n",
    "            train_step(batch_inputs, batch_targets)\n",
    "        template = '\\r Epoch {}, Train Loss: {}, Train Accuracy: {}'\n",
    "        print(template.format(epoch, \n",
    "                              train_loss.result(), \n",
    "                              train_accuracy.result()*100), end=\"\")\n",
    "        model.reset_states() #On reinitialise le state pour la prochaine epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96U-Sax_EZMx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uosm5_Mh2kYf"
   },
   "source": [
    "# --> Sauveguarde du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fufjMEiP2kYg"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "size_poetries = 300\n",
    "\n",
    "poetries = np.zeros((batch_size, size_poetries, 1))\n",
    "sequences = np.zeros((batch_size, 100))\n",
    "for b in range(batch_size):\n",
    "    rd = np.random.randint(0, len(inputs) - 100)\n",
    "    sequences[b] = inputs[rd:rd+100]\n",
    "\n",
    "for i in range(size_poetries+1):\n",
    "    if i > 0:\n",
    "        poetries[:,i-1,:] = sequences\n",
    "    softmax = predict(sequences)\n",
    "    # Set the next sequences\n",
    "    sequences = np.zeros((batch_size, 1))\n",
    "    for b in range(batch_size):\n",
    "        argsort = np.argsort(softmax[b][0])\n",
    "        argsort = argsort[::-1]\n",
    "        # Select one of the strongest 4 proposals\n",
    "        sequences[b] = argsort[0]\n",
    "\n",
    "for b in range(batch_size):\n",
    "    sentence = \"\".join([int_to_vocab[i[0]] for i in poetries[b]])\n",
    "    print(sentence)\n",
    "    print(\"\\n=====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lrDeky22kYg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
