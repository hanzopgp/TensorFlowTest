{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karna\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel\\parentpoller.py:113: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  warnings.warn(\"\"\"Parent poll failed.  If the frontend dies,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> Importation dataset poeme de Victor Hugo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du text :  127286\n",
      "Texte avant preprocessing :\n",
      " Parce que, jargonnant vêpres, jeûne et vigile,\n",
      "Exploitant Dieu qui rêve au fond du firmament,\n",
      "Vous a\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../Datasets/VictorHugoPoems/victorhugo.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(\"Taille du text : \", len(text))\n",
    "print(\"Texte avant preprocessing :\\n\", text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> Preprocessing du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire :  57\n",
      "Vocabulaire :\n",
      " {'R', 'j', 'P', 'D', 'z', 'w', 'o', 'n', 'p', 'B', 'X', 'S', 't', 'T', 'u', 'q', 'L', 'I', 'J', 'E', 'K', 'V', 'Y', 'k', 'r', '\\n', 'd', 'A', 'F', ',', 'Q', 'h', 'b', 'a', 'e', 's', 'i', 'U', 'O', '.', 'x', 'M', 'G', 'g', 'C', 'y', 'f', '\"', 'H', \"'\", ':', 'l', ' ', 'm', 'N', 'c', 'v'}\n",
      "Texte formate :\n",
      " Parce que, jargonnant vepres, jeune et vigile,\n",
      "Exploitant Dieu qui reve au fond du firmament,\n",
      "Vous a\n"
     ]
    }
   ],
   "source": [
    "#Supprime les caracteres inutiles, les majuscules...\n",
    "import unidecode\n",
    "text = unidecode.unidecode(text)\n",
    "text.lower()\n",
    "text = text.replace(\"2\", \"\")\n",
    "text = text.replace(\"1\", \"\")\n",
    "text = text.replace(\"8\", \"\")\n",
    "text = text.replace(\"5\", \"\")\n",
    "text = text.replace(\">\", \"\")\n",
    "text = text.replace(\"<\", \"\")\n",
    "text = text.replace(\"!\", \"\")\n",
    "text = text.replace(\"?\", \"\")\n",
    "text = text.replace(\"-\", \"\")\n",
    "text = text.replace(\"$\", \"\")\n",
    "text = text.replace(\";\", \"\")\n",
    "text = text.strip()\n",
    "\n",
    "#Supprime tous les doublons\n",
    "vocab = set(text) \n",
    "\n",
    "#Affichage resultat\n",
    "print(\"Taille du vocabulaire : \", len(vocab))\n",
    "print(\"Vocabulaire :\\n\", vocab)\n",
    "print(\"Texte formate :\\n\", text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab to int :\n",
      " {'R': 0, 'j': 1, 'P': 2, 'D': 3, 'z': 4, 'w': 5, 'o': 6, 'n': 7, 'p': 8, 'B': 9, 'X': 10, 'S': 11, 't': 12, 'T': 13, 'u': 14, 'q': 15, 'L': 16, 'I': 17, 'J': 18, 'E': 19, 'K': 20, 'V': 21, 'Y': 22, 'k': 23, 'r': 24, '\\n': 25, 'd': 26, 'A': 27, 'F': 28, ',': 29, 'Q': 30, 'h': 31, 'b': 32, 'a': 33, 'e': 34, 's': 35, 'i': 36, 'U': 37, 'O': 38, '.': 39, 'x': 40, 'M': 41, 'G': 42, 'g': 43, 'C': 44, 'y': 45, 'f': 46, '\"': 47, 'H': 48, \"'\": 49, ':': 50, 'l': 51, ' ': 52, 'm': 53, 'N': 54, 'c': 55, 'v': 56}\n",
      "Int to vocab :\n",
      " {0: 'R', 1: 'j', 2: 'P', 3: 'D', 4: 'z', 5: 'w', 6: 'o', 7: 'n', 8: 'p', 9: 'B', 10: 'X', 11: 'S', 12: 't', 13: 'T', 14: 'u', 15: 'q', 16: 'L', 17: 'I', 18: 'J', 19: 'E', 20: 'K', 21: 'V', 22: 'Y', 23: 'k', 24: 'r', 25: '\\n', 26: 'd', 27: 'A', 28: 'F', 29: ',', 30: 'Q', 31: 'h', 32: 'b', 33: 'a', 34: 'e', 35: 's', 36: 'i', 37: 'U', 38: 'O', 39: '.', 40: 'x', 41: 'M', 42: 'G', 43: 'g', 44: 'C', 45: 'y', 46: 'f', 47: '\"', 48: 'H', 49: \"'\", 50: ':', 51: 'l', 52: ' ', 53: 'm', 54: 'N', 55: 'c', 56: 'v'}\n"
     ]
    }
   ],
   "source": [
    "#On traduit maintenant tout le vocabulaire en nombre\n",
    "vocab_size = len(vocab)\n",
    "#Dictionnaire traduction\n",
    "vocab_to_int = {l:i for i,l in enumerate(vocab)} \n",
    "int_to_vocab = {i:l for i,l in enumerate(vocab)}\n",
    "#Affichage\n",
    "print(\"Vocab to int :\\n\", vocab_to_int)\n",
    "print(\"Int to vocab :\\n\", int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 33, 24, 55, 34, 52, 15, 14, 34, 29, 52, 1, 33, 24, 43, 6, 7, 7, 33, 7, 12, 52, 56, 34, 8, 24, 34, 35, 29, 52, 1, 34, 14, 7, 34, 52, 34, 12, 52, 56, 36, 43, 36, 51, 34, 29, 25, 19, 40, 8, 51, 6, 36, 12, 33, 7, 12, 52, 3, 36, 34, 14, 52, 15, 14, 36, 52, 24, 34, 56, 34, 52, 33, 14, 52, 46, 6, 7, 26, 52, 26, 14, 52, 46, 36, 24, 53, 33, 53, 34, 7, 12, 29, 25, 21, 6, 14, 35, 52, 33]\n"
     ]
    }
   ],
   "source": [
    "#Le dictionnaire nous permet de traduire notre text en nombre\n",
    "encoded = [vocab_to_int[l] for l in text]\n",
    "encoded_sentence = encoded[:100]\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'a', 'r', 'c', 'e', ' ', 'q', 'u', 'e', ',', ' ', 'j', 'a', 'r', 'g', 'o', 'n', 'n', 'a', 'n', 't', ' ', 'v', 'e', 'p', 'r', 'e', 's', ',', ' ', 'j', 'e', 'u', 'n', 'e', ' ', 'e', 't', ' ', 'v', 'i', 'g', 'i', 'l', 'e', ',', '\\n', 'E', 'x', 'p', 'l', 'o', 'i', 't', 'a', 'n', 't', ' ', 'D', 'i', 'e', 'u', ' ', 'q', 'u', 'i', ' ', 'r', 'e', 'v', 'e', ' ', 'a', 'u', ' ', 'f', 'o', 'n', 'd', ' ', 'd', 'u', ' ', 'f', 'i', 'r', 'm', 'a', 'm', 'e', 'n', 't', ',', '\\n', 'V', 'o', 'u', 's', ' ', 'a']\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = [int_to_vocab[i] for i in encoded_sentence]\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parce que, jargonnant vepres, jeune et vigile,\n",
      "Exploitant Dieu qui reve au fond du firmament,\n",
      "Vous a\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = \"\".join(decoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> Creation des batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First inputs :  [2, 33, 24, 55, 34, 52, 15, 14, 34, 29]\n",
      "First targets :  [33, 24, 55, 34, 52, 15, 14, 34, 29, 52]\n"
     ]
    }
   ],
   "source": [
    "#Un batch = plusieurs sequences de mots\n",
    "#Ce qu'on peut faire lorsqu'on a un dataset comme cela, on peut prendre une sequence de quelques mots\n",
    "#Chaque lettre est une entree dont le target est la lettre suivante. \n",
    "#Une incoherence peut arriver lors de l'analyse de la premiere lettre d'une sequence\n",
    "#Car dans notre cellule RNN il n'a pas d'informations sur la lettre precedente car la memoire est nulle.\n",
    "#Au lieu de lui mettre un etat nulle on lui mets l'etat retenu du batch precendent.\n",
    "#On ne peut donc pas se permettre de selectionner des sequences aleatoires dans notre texte.\n",
    "#On va donc seprarer notre texte en chunks\n",
    "#Une epoch : un ensemble de batch\n",
    "def gen_batch(inputs, targets, seq_len, batch_size, noise=0):\n",
    "    \n",
    "    chunk_size = (len(inputs) -1) // batch_size\n",
    "    sequences_per_chunk = chunk_size // seq_len\n",
    "    \n",
    "    for seq in range(0, sequences_per_chunk):\n",
    "        batch_inputs = np.zeros((batch_size, seq_len))\n",
    "        batch_targets = np.zeros((batch_size, seq_len))\n",
    "        for b in range(0, batch_size):\n",
    "            fr = (b*chunk_size) + (seq*seq_len)\n",
    "            to = fr + seq_len\n",
    "            batch_inputs[b] = inputs[fr:to]\n",
    "            batch_targets[b] = inputs[fr+1:to+1]\n",
    "\n",
    "            if noise > 0: #\"noise\" aide le model a generaliser, evite l'overfitting\n",
    "                noise_indices = np.random.choice(seq_len, noise)\n",
    "                batch_inputs[b][noise_indices] = np.random.randint(0, vocab_size)\n",
    "\n",
    "        yield batch_inputs, batch_targets #Permet d'appeler la fonction dans la boucle\n",
    "            \n",
    "inputs, targets = encoded, encoded[1:]\n",
    "print(\"First inputs : \", inputs[:10])\n",
    "print(\"First targets : \", targets[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################### Sans noise #####################\n",
      "\n",
      "----------------------Step  1 ----------------------\n",
      "\n",
      "Batch input :\n",
      " (32, 5) \n",
      "Batch target shape :\n",
      " (32, 5)\n",
      "\n",
      "Batch input :\n",
      " [ 2. 33. 24. 55. 34.] \n",
      "Batch target :\n",
      " [33. 24. 55. 34. 52.]\n",
      "\n",
      "----------------------Step  2 ----------------------\n",
      "\n",
      "Batch input :\n",
      " (32, 5) \n",
      "Batch target shape :\n",
      " (32, 5)\n",
      "\n",
      "Batch input :\n",
      " [52. 15. 14. 34. 29.] \n",
      "Batch target :\n",
      " [15. 14. 34. 29. 52.]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n##################### Sans noise #####################\")\n",
    "i = 0\n",
    "for batch_inputs, batch_targets in gen_batch(inputs, targets, seq_len=5, batch_size=32, noise=0): #Sequence de 5, batch de 32\n",
    "    i += 1\n",
    "    print(\"\\n----------------------Step \", i, \"----------------------\")\n",
    "    print(\"\\nBatch input :\\n\", batch_inputs.shape, \"\\nBatch target shape :\\n\", batch_targets.shape)\n",
    "    print(\"\\nBatch input :\\n\", batch_inputs[0], \"\\nBatch target :\\n\", batch_targets[0])\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################### Avec noise = 3 #####################\n",
      "\n",
      "---------------------- Step  1  ----------------------\n",
      "\n",
      "Batch input :\n",
      " (32, 5) \n",
      "Batch target shape :\n",
      " (32, 5)\n",
      "\n",
      "Batch input :\n",
      " [ 2. 33. 24. 47. 47.] \n",
      "Batch target :\n",
      " [33. 24. 55. 34. 52.]\n",
      "\n",
      "---------------------- Step  2  ----------------------\n",
      "\n",
      "Batch input :\n",
      " (32, 5) \n",
      "Batch target shape :\n",
      " (32, 5)\n",
      "\n",
      "Batch input :\n",
      " [52. 55. 55. 34. 29.] \n",
      "Batch target :\n",
      " [15. 14. 34. 29. 52.]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n##################### Avec noise = 3 #####################\")\n",
    "i = 0\n",
    "for batch_inputs, batch_targets in gen_batch(inputs, targets, seq_len=5, batch_size=32, noise=3): #Sequence de 5, batch de 32\n",
    "    i += 1\n",
    "    print(\"\\n---------------------- Step \", i, \" ----------------------\")\n",
    "    print(\"\\nBatch input :\\n\", batch_inputs.shape, \"\\nBatch target shape :\\n\", batch_targets.shape)\n",
    "    print(\"\\nBatch input :\\n\", batch_inputs[0], \"\\nBatch target :\\n\", batch_targets[0])\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Les valeurs au dessus ne nous convienne pas pour entrainer un model il y a mieux.\n",
    "#On va donc utiliser le one hot encoding pour simplifier la tache à notre model.\n",
    "#Exemple de one hot encoding : a => 2 => [0, 1, 0, 0]\n",
    "#Le one hot encoding est tres efficace lorsqu'on veut specifier des classes.\n",
    "#En effet, il n'y a aucune raison qu'un nombres que nous donnons a un caractere\n",
    "#ait un nombre plus eleve et donc avec plus de poids qu'un autre alors qu'il n'y\n",
    "#a aucune hierarchie entre les caracteres.\n",
    "class OneHot(tf.keras.layers.Layer): #On creer une custom layer OneHot\n",
    "    \n",
    "    def __init__(self, depth, **kwargs):\n",
    "        super(OneHot, self).__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "         #Transforme le x en int 32 et creer un vecteur one hot encoded\n",
    "        return tf.one_hot(tf.cast(x, tf.int32), self.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input letter :\n",
      " 2.0\n",
      "Next letter prediction :\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "class RNNModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.one_hot = OneHot(len(vocab))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        output_layer = self.one_hot(inputs)\n",
    "        return output_layer\n",
    "    \n",
    "batch_inputs, batch_targets = next(gen_batch(inputs, targets, seq_len=50, batch_size=32)) #32 sequences, 50 elements\n",
    "model = RNNModel(len(vocab))\n",
    "output = model.predict(batch_inputs)[0][0]\n",
    "\n",
    "print(\"Input letter :\\n\", batch_inputs[0][0])\n",
    "print(\"Next letter prediction :\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> Creation du model RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "#Input layer\n",
    "#On ne set pas le nombre d'element dans les sequences\n",
    "tf_inputs = tf.keras.Input(shape=(None,), batch_size=32) \n",
    "#One hot layer\n",
    "#En lui passant tf_inputs, on specifie la shape qu'on enverra dans la layer one_hot\n",
    "one_hot = OneHot(vocab_size)(tf_inputs) \n",
    "#LSTM layers\n",
    "#\"return_sequences\" permet de specifier que l'on prend en compte plusieurs des anciennes\n",
    "#cellules LSTM, si on met a False nous aurions que l'information de la derniere cellule LSTM\n",
    "#\"stateful\" permet de specifier qu'a chaque appel on ne va pas reinitialiser les cellules.\n",
    "#A chaque appel l'etat initial sera egal au dernier element de la sequence precedente\n",
    "rnn_layer1 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(one_hot) \n",
    "rnn_layer2 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(rnn_layer1)\n",
    "#Dense layer\n",
    "hidden_layer = tf.keras.layers.Dense(128, activation=\"relu\")(rnn_layer2)\n",
    "#Output layer\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "#Model\n",
    "model = tf.keras.Model(inputs=tf_inputs, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First prediction :\n",
      " [0.01749277 0.01761051 0.01752927 0.01753522 0.01757139 0.01755441\n",
      " 0.01750322 0.01750031 0.0175569  0.01763936 0.01750557 0.01754783\n",
      " 0.01760262 0.01756444 0.01751658 0.01755176 0.01764033 0.01752071\n",
      " 0.01751695 0.01750359 0.01751272 0.0175317  0.01751729 0.01750191\n",
      " 0.01756243 0.01758251 0.0175095  0.01755666 0.01758123 0.01766211\n",
      " 0.01755156 0.01764601 0.01752544 0.01749381 0.01751448 0.01751726\n",
      " 0.01757346 0.01758179 0.01745911 0.01751097 0.01749659 0.0175242\n",
      " 0.01755689 0.01753224 0.01757489 0.01756098 0.01751489 0.01758607\n",
      " 0.01751515 0.01752996 0.01753598 0.01756204 0.01751779 0.01757111\n",
      " 0.0175326  0.0175133  0.01751969]\n",
      "Second prediction :\n",
      " [0.01749277 0.01761051 0.01752927 0.01753522 0.01757139 0.01755441\n",
      " 0.01750322 0.01750031 0.0175569  0.01763936 0.01750557 0.01754783\n",
      " 0.01760262 0.01756444 0.01751658 0.01755176 0.01764033 0.01752071\n",
      " 0.01751695 0.01750359 0.01751272 0.0175317  0.01751729 0.01750191\n",
      " 0.01756243 0.01758251 0.0175095  0.01755666 0.01758123 0.01766211\n",
      " 0.01755156 0.01764601 0.01752544 0.01749381 0.01751448 0.01751726\n",
      " 0.01757346 0.01758179 0.01745911 0.01751097 0.01749659 0.0175242\n",
      " 0.01755689 0.01753224 0.01757489 0.01756098 0.01751489 0.01758607\n",
      " 0.01751515 0.01752996 0.01753598 0.01756204 0.01751779 0.01757111\n",
      " 0.0175326  0.0175133  0.01751969]\n"
     ]
    }
   ],
   "source": [
    "#Reset les cellules du RNN\n",
    "model.reset_states()\n",
    "\n",
    "#Creer un premier batch\n",
    "batch_inputs, target_inputs = next(gen_batch(inputs, targets, seq_len=50, batch_size=32))\n",
    "\n",
    "#Prediction pour un premier batch\n",
    "outputs = model.predict(batch_inputs)\n",
    "#Prediction de la premiere sortie\n",
    "first_prediction = outputs[0][0]\n",
    "print(\"First prediction :\\n\", first_prediction)\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "#Deuxieme prediction, c'est exactement la meme grace au stateful=True\n",
    "outputs = model.predict(batch_inputs)\n",
    "second_prediction = outputs[0][0]\n",
    "print(\"Second prediction :\\n\", second_prediction)\n",
    "\n",
    "#Check si les deux predictions sont egales avec un reset_state() entre les deux\n",
    "assert(set(first_prediction)==set(second_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001) #lr : learning rate\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #Fait une prediction sur le batch\n",
    "        predictions = model(inputs)\n",
    "        #Recupere l'erreur par rapport aux predictions faites\n",
    "        loss = loss_object(targets, predictions)\n",
    "    #Calcul du gradient\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    #Change les poids du model grace au gradient\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    #Garde l'information sur l'evolution de l'entrainement\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets, predictions)\n",
    "\n",
    "@tf.function\n",
    "def predict(inputs):\n",
    "    # Fait une prediction sur tous le batch\n",
    "    predictions = model(inputs)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> Entrainement du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(32, None)]              0         \n",
      "_________________________________________________________________\n",
      "one_hot_1 (OneHot)           (32, None, 57)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (32, None, 128)           95232     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (32, None, 128)           131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 128)           16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (32, None, 57)            7353      \n",
      "=================================================================\n",
      "Total params: 250,681\n",
      "Trainable params: 250,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Epoch 9, Train Loss: 2.7581796646118164, Train Accuracy: 22.476282119750977"
     ]
    }
   ],
   "source": [
    "model.reset_states()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "for epoch in range(4000):\n",
    "    #Pendant toute cette etape dans le for, on ne reinitialise pas les states\n",
    "    for batch_inputs, batch_targets in gen_batch(inputs, targets, seq_len=100, batch_size=32, noise=13): #Sequence de taille 100, batch de 32\n",
    "        train_step(batch_inputs, batch_targets)\n",
    "    template = '\\r Epoch {}, Train Loss: {}, Train Accuracy: {}'\n",
    "    print(template.format(epoch, \n",
    "                          train_loss.result(), \n",
    "                          train_accuracy.result()*100), end=\"\")\n",
    "    model.reset_states() #On reinitialise le state pour la prochaine epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> Sauveguarde du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "model.save(\"model_rnn.h5\")\n",
    "\n",
    "with open(\"model_rnn_vocab_to_int\", \"w\") as f:\n",
    "    f.write(json.dumps(vocab_to_int))\n",
    "with open(\"model_rnn_int_to_vocab\", \"w\") as f:\n",
    "    f.write(json.dumps(int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
