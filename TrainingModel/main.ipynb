{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> Creation d'un model simple\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> Setup optimizer, loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy() #Loss function\n",
    "optimizer = tf.keras.optimizers.Adam()                        #Optimizer\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')         #Track l'evolution du model avec l'accumulateur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> Definir l'operation d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(image, targets):\n",
    "    #with tf.GradientTape() permet de calculer toutes les operations qui ont ete faite\n",
    "    #Ce qu'il y a a l'interieur de l'indexation est capture pour ensuite calculer\n",
    "    #le gradient.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(image)               #\"predictions\" est un tensor\n",
    "        loss = loss_object(targets, predictions) #pour pouvoir utiliser loss_object().\n",
    "    #On calcule le gradient avec les variables accessible par model.trainable_variables.\n",
    "    #On recupere aussi le loss pour le calculer.\n",
    "    gradients = tape.gradient(loss, model.trainable_variables) \n",
    "    for gradient in gradients:\n",
    "        print(\"Gradients shape : \", gradient.shape) #(30, 256) Il y a 256 neurones sur la couche intermediaire et 30 entrees.\n",
    "                                                    #(256,) Valeurs des biais de l'hidden layer.\n",
    "                                                    #(256, 128) Il y a 256 neurones sur la couche intermediaire et 128 .\n",
    "                                                    #sur la couche intermediaire 2.\n",
    "                                                    #(128,) Valeurs des biais de l'hidden layer 2.\n",
    "                                                    #(128, 2) Il y a 128 neurones sur la couche intermediaire 2 et 2 sorties.\n",
    "                                                    #(2,) Valeurs des biais de la layer de sortie .\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables)) #On applique le gradient a l'optimizer Adam.\n",
    "                                                                         #zip() permet de faire l'association entre les\n",
    "                                                                         #gradients et les variables.\n",
    "    train_loss(loss) #On continue de tracker l'evolution du loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> Generation de fausse donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients shape :  (30, 256)\n",
      "Gradients shape :  (256,)\n",
      "Gradients shape :  (256, 128)\n",
      "Gradients shape :  (128,)\n",
      "Gradients shape :  (128, 2)\n",
      "Gradients shape :  (2,)\n",
      "Loss : tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Loss : tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Loss : tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Loss : tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Loss : tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Loss : tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Loss : tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Loss : tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Loss : tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "Loss : tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 10):\n",
    "    for _ in range(0, 100):\n",
    "        #On cree des faux inputs\n",
    "        inputs = np.zeros((2, 30))\n",
    "        inputs[0] -= 1\n",
    "        inputs[1] = 1\n",
    "        #On cree les faux targets\n",
    "        targets = np.zeros((2, 1))\n",
    "        targets[0] = 0\n",
    "        targets[1] = 1\n",
    "        #On train le model avec ces fausses donnees\n",
    "        train_step(inputs, targets)\n",
    "    #train_loss(0.0001) #On peut ajouter des valeurs pour verifier que le result() fonctionne bien\n",
    "    print(\"Loss : %s\" % train_loss.result()) #Affiche la moyenne de toutes les erreurs stockees a l'interieur de train_loss\n",
    "    train_loss.reset_states() #Vide le buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu] *",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
